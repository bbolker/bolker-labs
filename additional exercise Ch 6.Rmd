---
title: "Additional exercise Ch. 6"
author: "Bob Douma"
date: "22 November 2017"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

## Fitting models to data

In this exercise you will learn how to fit models to data through means of maximum likelihood and compare the likelihood of different models (hypotheses). Fitting a model to data through likelihood requires that you take four steps: 

1. Specify how the dependent variable depends on the independent variable, i.e. specify a function how the mean of y depends on the value of x.
2. Specify a probability distribution to describe the deviations of the observations from the mean
3. Choose the parameters of the deterministic model and the probability model such that the negative log likelihood is lowest.
4. Compare the likelihood of alternative models (change the deterministic function or the stochastic function) and compare with AIC(c) or BIC which model is most parsimonious. 

To fit a model through means of maximum likelihood you need to specify a function that calculate the negative log likelihood (NLL) based on the data and the parameter values. For example to calculate the NLL of a linear model and a normal distribution the following function works: 


```{r}
nll = function(par,y,x){
  a = par[1]
  b = par[2]
  sd = par[3]
  # this calculates the mean y for a given value of x: the deterministic function
  mu = a+b*x 
  # this calculates the likelihood of the function given the probability 
  # distribution, the data and mu and sd
  nll = -sum(dnorm(y,mean=mu,sd=sd,log=T))
  return(nll)
}

```
Note that `-sum(`**`log`**`(dnorm(y,mean=mu,sd=sd)))` should not be used as it may lead to underflow (the computer cannot store very very small probabilities) and therefore to optimisation problems. 

Next we specify a function to find the maximum likelihood estimate

```{r, eval=F}
par=c(a=1,b=1,c=1) # initial parameters
opt1 = optim(par=par,nll,x=x,y=y) # y represents the data, x the independent variable
```

It can also be done through `mle2`
```{r}
nll.mle = function(a,b,sd){
  # this calculates the mean y for a given value of x: the deterministic function
  mu = a+b*x 
  # this calculates the likelihood of the function given the probability 
  # distribution, the data and mu and sd
  nll = -sum(dnorm(y,mean=mu,sd=sd,log=T)) 
  return(nll)
}

```

```{r, eval=F}
# the data should be supplied through data and the parameters through list().
mle2.1 = mle2(nll.mle,start=list(a=1,b=1,sd=1),data=data.frame(x,y)) 
summary(mle2.1)
```


The following steps will lead you through the model fitting procedure. 

1. Take the first dataset and tweak the above functions such that it matches with the deterministic and stochastic model that you have chosen. In case you got stuck in the previous exercises where you had to choose a deterministic function and a stochastic function see next page for suggestions. 

      _hint_: In a previous exercise you have eyeballed the parameter values of the functions, you can use these as starting       values.
  
      _hint_: In case you get convergence problems, further adapt your starting values, or choose a different optimizer. For       example Nelder-Mead is a robust one, e.g. `method = "Nelder-Mead"`.

2. Change the determinstic function for a possible alternative determinstic function
3. Compare the likelihoods of the data given both models
4. Apply model selection criteria and conclude which model fits that data best. 
5. Does the model makes sense from a biological perspective?
      
Optional and time permitting:      
6. Repeat the above procedure for the other 5 datasets

The next exercise will be about assessing the uncertainty in the estimated parameters. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the one based on the Hessian matrix. We take a number of steps to get a feel for the uncertainty in the parameter estimates.

Take the first dataset and assume that the function was generated by the monomolecular function $a(1-e^{(-bx)}$. Fit this model with normally distributed errors through this data with `mle2` and inspect the coefficients.

1. Calculate for the parameters $a$ and $b$ the NLL for a number of combinations of $a$ and $b$ and plot the NLL surface using `contour` plot. What is your conclusion about the relationship between the two parameters? Are they fully independent?
  _hint_: you can use a for a double for-loop to run over all parameters
  _hint_: check Lab 6 of Bolker for inspiration how to do this.
  
2.  Calculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile and through the quadratic approximation. Use the following pseudocode to achieve this:
  a. Adapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter $a$. 
  b .Vary $a$ of a range and optimise the other parameteters. 
  c. Plot the NLL as a function of parameter $a$.
  d. Find the values of $a$ that enclose $-L + \chi^2(1-\alpha)/2$
  e. Compare your results with the function `confint()`
  
3. Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:
  a. Get the standard error of the parameter estimates through `vcov`. Note that `vcov` return the variance/covariance matrix
  b. Calculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.

4. Plot the confidence limits of the both method and compare the results. What is your conclusion?

```{r,eval=F,echo=F}
shapes1= read.csv("D:/BobDouma/Education/CSA-34306 Ecological Models and Data in R/tutorials/repos/bolker-labs/shapes1.csv")
plot(shapes1)

nll.mle = function(a,b,sd){
  # this calculates the mean y for a given value of x: the deterministic function
  mu = a*(1-exp(-b*x))
  # this calculates the likelihood of the function given the probability 
  # distribution, the data and mu and sd
  nll = -sum(dnorm(y,mean=mu,sd=sd,log=T)) 
  return(nll)
}


mle2.1 = mle2(nll.mle,start=list(a=25,b=0.0080,sd=1),data=data.frame(x=shapes1$x,y=shapes1$y),method="Nelder-Mead") 
summary(mle2.1)
logLik(mle2.1)
confint(mle2.1)
coefficients(mle2.1)

plot(shapes1)
curve(coef(mle2.1)[1]*(1-exp(-coef(mle2.1)[2]*x)),add=T)

```
```{r,eval=F,echo=F}
# likelihood surface
a1 = seq(10,30,length.out = 200)
b1= seq(0.003,0.02,length.out=200)

nll.grid = expand.grid(a1,b1)
nll.grid$NLL = NA
no = 0
for (i in 1:length(a1)){
  for (j in 1:length(b1)){
    no = no + 1
    nll.grid[no,1] = a1[i]
    nll.grid[no,2] = b1[j]
    nll.grid[no,3] = nll.mle(a=a1[i],b=b1[j],sd=2.06)
  }
}
z1 = as.matrix(dcast(nll.grid,Var1~Var2)[,-1])
contour(b1,a1,z1,nlevels=60)

# profile
nll.mle1 = function(a,sd){
  # this calculates the mean y for a given value of x: the deterministic function
  mu = a*(1-exp(-b*x))
  # this calculates the likelihood of the function given the probability 
  # distribution, the data and mu and sd
  nll = -sum(dnorm(y,mean=mu,sd=sd,log=T)) 
  return(nll)
}

nll = numeric(length(b1))
for (i in 1:length(b1)){
  b = b1[i]
  mle.21 = mle2(nll.mle1,start=list(a=25,sd=7.96),data=data.frame(x=shapes1$x,y=shapes1$y),method="Nelder-Mead")   
  nll[i] = -logLik(mle.21)
}
plot(nll~ b1,type="l",xlim=c(0.008,0.012))
which.min(nll)

# cutoff
-logLik(mle2.1) + qchisq(0.95, 1)/2
which(nll < 119.852)
b1[c(53,95)]

plot(nll~ b1,type="l",xlim=c(0.007,0.0110),ylim=c(116,125))
abline(v=c(0.00744,0.01103),lty=2)
abline(v=0.008968,lty=1,lwd=2)
abline(v=c(0.00738,0.01103),lty=2,col="red")

c = mle2.1@details$hessian/2
b = coef(mle2.1)[2]
a = -logLik(mle2.1)

se.mu = sqrt(diag(solve(mle2.1@details$hessian))[2])
b + c(-1,1)*qnorm(0.975) * se.mu
confint(mle2.1)

abline(v=c(0.007177,0.0107589),col="blue")

```

\newpage
## Hints for choosing deterministic functions and stochastic functions
  
  1. Deterministic functions

    **dataset 1**
    light response curve. There are a number of options of functions to choose from, depending on the level of sophistication:
    $\frac{ax}{(b+x)}$, $a(1-e^{(-bx)})$, $\frac{1}{2\theta}(\alpha I+p_{max}-\sqrt(\alpha I+p_{max})^2-4\theta I p_{max})$ see page 98. A parameter `d` can be added in all cases to shift the curve up or down.
      
    **dataset 2**
    The dataset describes a functional responses. Bolker mentions four of those $min(ax,s)$ $\frac{ax}{(b+x)}$, $\frac{ax^2}{(b^2+x^2)}$,$\frac{ax^2}{(b+cx+x^2)}$
      
    **dataset 3**
    Allometric relationships generally have the form $ax^b$
      
      **dataset 4**
      This could be logistic growth $n(t)=\frac{K}{1+(\frac{K}{n_0})e^{-rt}}$ or the gompertz function $f(x)=e^{-ae^{-bx}}$
      
      **dataset 5**
      What about a negative exponential? $ae{-bx}$ or a power function $ax^b$
      
      **dataset 6**
      Species reponse curves are curves that describe the probability of presence as a function of some factor. A good candidate good be a unimodel response curve. You could take the equation of the normal distribution without the scaling constant: e.g.
      $a e^{\frac{-(x-\mu)^2}{2\sigma^2}}$

  2. Stochastic functions/Probability distributions

      **dataset 1**
      y represents real numbers and both positive and negative numbers occur. This implies that we should choose a continuous         probability distribution. In addition, the numbers seems unbound. Within the family of continuous probability distributions, the normal seems a good candidate distribution because this one runs from -$\inf$ to +$\inf$. In contrast the Gamma and the Lognormal only can take positive numbers, so these distributions cannot handle the negative numbers. In addition, the beta distribution is not a good candidate because it runs from 0-1.

      **dataset 2**
      y represents real numbers and only positive numbers occur. The data represents a functional response (intake rate of the predator), and it is likely that you can only measure positive numbers (number of prey items per unit of time).  This implies that we should choose a continuous probability distribution. Within the family of continuous probability distributions, the Gamma and the Lognormal could be taken as candidate distributions because they can only take positive numbers (beware that the Gamma cannot take 0). However, you could try to use a normal as well.

      **dataset 3**
      y seems represents counts (this is the cone dataset that is introduced in ch. 6.). Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.
      
      **dataset 4**
      y represents population size over time. From looking at the data, they seems to represent counts. Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.  

      **dataset 5**
      No information is given on y. The data clearly seems to represent counts. Thus the same reasoning applies here as to the two previous datasets.
      
      **dataset 6**
      The data (y) represents species occurences (presence/absence). The binomial model would be a good model to predict the probability of presence.  