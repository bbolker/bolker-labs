---
title: "Lab 4: probability distributions, averaging and Jensen's inequality"
author: \copyright 2005 Ben Bolker, modified at some places by Alejandro Morales & Bob Douma 2017
date: "November 7, 2017"
geometry: margin=4cm
fontsize: 11pt
output: pdf_document
---

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=5), tidy = TRUE)
```


This lab has two parts. A part to make you familiar with the technicalities of stochastic distributions in `R`: how to generate values from probability distributions; how to make your own probability distribution etc (section 1-5), and a part (section 6) to help you practicing with probability distributions on different types of data.  Under time constraints, make sure that you make at least make the exercise in section 1 and section 6. 

# 1. Random distributions in `R`
`R` knows about lots of probability distributions.  For each, it
can generate random numbers drawn from the distribution ("deviates"); compute the cumulative distribution function and the probability distribution function; and compute the quantile function, which gives the
$x$ value such that $\int_0^x P(x) \, dx$ (area under the curve from 0 to $x$) is a specified value, such as 0.95 (think about "tail areas" from standard statistics).


<!-- ```{r,echo=FALSE,fig.cap="`R` functions for an arbitrary distribution `dist`, showing density function (`ddist`), cumulative distribution function (`pdist`), quantile function (`qdist`), and random-deviate function (`rdist`).", fig.height = 3.5} -->
<!-- sh = 4 -->
<!-- textsize = 1.2 -->
<!-- op=par(mfrow=c(1,2),mar=c(5.1,5.5,0,0.5)) -->
<!-- curve(dgamma(x,shape=sh),from=0,to=20,ylab="",lwd=2,axes=FALSE) -->
<!-- axis(side=1,labels=FALSE) -->
<!-- axis(side=2,labels=FALSE) -->
<!-- box() -->
<!-- xvec = seq(0,5,length=100) -->
<!-- polygon(c(xvec,rev(xvec)),c(rep(0,length(xvec)),dgamma(rev(xvec),shape=sh)),col="gray",border=NA) -->
<!-- curve(dgamma(x,shape=sh),from=0,to=20,lwd=2,add=TRUE) -->
<!-- abline(v=5,lty=3) -->
<!-- mtext(side=1,line=1.8,at=5,expression(x[0]),cex=textsize) -->
<!-- abline(h=dgamma(5,shape=sh),lty=2,lwd=2) -->
<!-- mtext(side=2,at=dgamma(5,shape=sh),las=1,expression(ddist(x[0])), -->
<!--       line=1.8,cex=textsize) -->
<!-- text(10,0.1,expression(pdist(x[0])),cex=textsize,col="darkgray") -->
<!-- mtext(side=2,at=0.0,adj=0,"Probability density",cex=textsize,line=3.5) -->
<!-- set.seed(1001) -->
<!-- points(rgamma(10,shape=sh),rep(0,10),cex=1.5) -->
<!-- text(11.7,0.03,"rdist(10)",adj=0,cex=textsize) -->
<!-- arrows(10.8,0.023,6.6,0.008,lwd=2) -->
<!-- curve(pgamma(x,shape=sh),from=0,to=20,ylab="",lwd=2,axes=FALSE) -->
<!-- axis(side=1,labels=FALSE) -->
<!-- axis(side=2,labels=FALSE) -->
<!-- box() -->
<!-- abline(v=5,lty=3) -->
<!-- mtext(side=1,line=1.8,at=5,expression(x[0]),cex=textsize) -->
<!-- abline(h=pgamma(5,shape=sh),lty=2,lwd=2) -->
<!-- mtext(side=2,at=pgamma(5,shape=sh),las=1,expression(pdist(x[0])), -->
<!--       line=1.8,cex=textsize) -->
<!-- abline(v=qgamma(0.95,shape=sh),lty=4,lwd=2) -->
<!-- mtext(side=2,at=0.95,las=1,0.95, -->
<!--       line=par("mgp")[2],cex=textsize) -->
<!-- segments(par("usr")[1],0.95,qgamma(0.95,shape=sh),0.95,lty=4,lwd=2) -->
<!-- mtext(side=1,at=qgamma(0.95,shape=sh),text="qdist(0.95)",line=1.8, -->
<!--       cex=textsize,adj=0.1) -->
<!-- mtext(side=2,at=-0.05,adj=0,"Cumulative distribution",cex=textsize,line=3.5) -->
<!-- ``` -->

```{r, echo=FALSE,fig.cap="`R` functions for the Normal distribution, showing probability density function (`dnorm`), and probability distribution (`pnorm`). Random samples are drawn  proportionally to the probability density function with `rnorm` and quantiles are calculated with by inverting the probability distribution with `qnorm`.", fig.height = 5}
par(mfrow = c(2,2), mar = c(2.5,4,2.5,1), mgp = c(3,1,0), las = 1, xaxs = "i", yaxs = "i")
mu = 10
sd = 3
x = 7.5
P = 0.4
# dnorm
curve(dnorm(x,mu,sd),0,20, ylab = "p(x)", main = "dnorm", ylim = c(0,0.15))
arrows(x,0,x,dnorm(x,mu,sd), length = 0.1, col = 2)
arrows(x,dnorm(x,mu,sd), 0, dnorm(x,mu,sd), length = 0.1, col = 2)

# rnorm
curve(dnorm(x,mu,sd),0,20, ylab = "p(x)", main = "rnorm", ylim = c(0,0.15))
set.seed(0)
sample = rnorm(20, mu, sd)
points(sample, rep(0.005, 20), cex = 1.5)

# pnorm
curve(pnorm(x,mu,sd),0,20, ylab = "P(x)", main = "pnorm", ylim = c(0,1))
arrows(x,0,x,pnorm(x,mu,sd), length = 0.1, col = 2)
arrows(x,pnorm(x,mu,sd), 0, pnorm(x,mu,sd), length = 0.1, col = 2)

# qnorm
curve(pnorm(x,mu,sd),0,20, ylab = "P(x)", main = "qnorm", ylim = c(0,1))
arrows(0,P, qnorm(P,mu,sd),P, length = 0.1, col = 2)
arrows(qnorm(P,mu,sd),P, qnorm(P,mu,sd), 0, length = 0.1, col = 2)
```

Let's take the binomial distribution (yet again) as an example.

- `rbinom(n,size,prob)` gives `n` random draws from the binomial
distribution with parameters `size` (total number of draws) and
`p` (probability of success on each draw).  
You can give different parameters for each draw.
For example:
```{r}
rbinom(10,size=8,prob=0.5)
rbinom(3,size=8,prob=c(0.2,0.4,0.6))
``` 
Figure 2 shows the result of drawing 200 values
from a binomial distribution with $N=12$ and $p=0.1$ and
plotting the results as a `factor` (with 200 draws
we don't have to worry about any of the 13 possible outcomes
getting missed and excluded from the plot):
```{r,eval=FALSE}
plot(factor(rbinom(200,size=12,prob=0.1)), xlab="# of successes", 
     ylab="# of trials out of 200")
``` 

```{r,fig=TRUE,echo=FALSE, fig.cap = "Results of `rbinom`", fig.height = 3}
par(mar = c(4.0,4.0,0.5,2))
plot(factor(rbinom(200,size=12,prob=0.1)),xlab="# of successes",
     ylab="# of trials out of 200")
``` 

- `dbinom(x,size,prob)` gives the value of the probability
distribution
function (pdf) at `x` (for a continous distribution,
the analogous function would compute the probability density
function). Since the binomial is discrete, `x`
has to be an integer, and the pdf is just the probability of getting
that many successes; if you try `dbinom` with a non-integer `x`, you'll get zero and a warning.

- `pbinom(q,size,prob)` gives the value of the cumulative
distribution
function (cdf) at `q` (e.g. `pbinom(7,size=10,prob=0.4)`);

- `qbinom(p,size,prob)` gives the quantile function $x=q(p)$,
where `p` is a number between 0 and 1 (an area under the pdf, or
value of the cdf) and $x$ is the value such that 
$P(X \le x)=p$.
The *quantile function* $Q$ is the inverse of the cumulative distribution
function $C$: if $Q(p)=q$ then $C(q)=p$.
Example: `qbinom(0.95,size=10,prob=0.4)`.


These four functions exist for each of the distributions `R` has built
in: e.g. for the normal distribution they're
`rnorm()`, `pnorm()`, `dnorm()`, `qnorm()`.
Each distribution has its own set of parameters (so e.g. `pnorm()`
is `pnorm(x,mean=0,sd=1)`). To see which distributions are available
in the `base` package; check `?distributions`.

**Exercise 1**:
For the binomial distribution with 10 trials and a success probability
of 0.2:

```{r,echo=FALSE,eval=FALSE}
set.seed(1001); rbinom(8,size=10,prob=0.2)
``` 
- Pick 8 random values and sort them into increasing order
(if you `set.seed(1001)` beforehand, you should get $X=0$
(twice), $X=2$ (4 times), and $X=4$ and $X=5$ (once each)).

- Calculate the probabilities of getting 3, 4, or 5
successes. Answer: 
```{r,echo=FALSE}
dbinom(3:5,size=10,prob=0.2)
``` 

- Calculate the probability of getting 5 or more
successes.
Answer: 
```{r,echo=FALSE}
pbinom(4,size=10,prob=0.2,lower.tail=FALSE)
``` 

- What tail values would you use to test against the (two-sided)
null hypothesis that `prob`$=0.2$ at the 95% level? (Use `qbinom()` to get the answer,
and use `pbinom(0:10,size=10,prob=0.2)` 
and `pbinom(0:10,size=10,prob=0.2,lower.tail=FALSE)` to check that your
answer makes sense.

You can use the `R` functions to test your understanding of
a distribution and make sure that random draws match up
with the theoretical distributions as they should.
This procedure is particularly valuable when you're developing new
probability distributions by combining simpler ones,
e.g. by zero-inflating or compounding distributions.

The results of a large number of random draws should have
the correct moments (mean and variance), and a histogram
of those random draws (with `freq=FALSE` or `prob=TRUE`)
should match up with the theoretical distribution.
For example, draws from a binomial distribution with
$p=0.2$ and $N=20$ should have a mean of approximately $Np=4$
and a variance of $Np(1-p)=3.2$:
```{r}
set.seed(1001)
N=20; p=0.2
x = rbinom(10000,prob=p,size=N)
c(mean(x),var(x))
``` 

The mean is very close, the variance
is a little bit farther off.
Just for the heck of it, we can use the
`replicate()` function to re-do this
command many times and see how close we get:
```{r}
var_dist = replicate(1000,var(rbinom(10000,prob=p,size=N)))
``` 
(this may take a little while; if it takes too long,
lower the number of replicates to 100).

Looking at the summary statistics and
at the 2.5% and 97.5% quantiles of the
distribution of variances:
```{r}
summary(var_dist)
quantile(var_dist,c(0.025,0.975))
``` 
(Try a histogram too.)
Even though there's some variation (of
the variance) around the theoretical value,
we seem to be doing the right thing since
the 95% confidence limits include the theoretical
value.
(Lab 5 will go into more detail on running
simulations to check the expected variation
of different measurement as a function
of parameters and sample size.)

```{r, eval = TRUE, echo = FALSE, fig = TRUE, fig.cap = "Checking binomial deviates against theoretical values."}
x = rbinom(10000,prob=p,size=N)
tx = table(factor(x,levels=0:12))/10000
b1 = barplot(tx,ylim=c(0,0.23),ylab="Probability")
points(b1,dbinom(0:12,prob=p,size=N),pch=16)
```

Finally, Figure 3 shows
the entire simulated frequency distribution
along with the theoretical values.
The steps in `R` are:

**1.** pick 10,000 random deviates:
```{r}
x = rbinom(10000,prob=p,size=N)
``` 

**2.** Tabulate the values, and divide
by the number of samples to get
a probability distribution:
```{r}
tx = table(factor(x,levels=0:12))/10000
``` 
(The `levels`
command is necessary in this case
because the probability of
$x=12$ with $p=0.2$ and $N=12$
is actually so low ($\approx 4\times 10^{-9}$)
that it's very unlikely that
a sample of 10,000 won't include any
samples with 12 successes.)

**3.** Draw a barplot of the values, extending
the $y$-limits a bit to make room for
the theoretical values and saving the
$x$ locations at which the bars are drawn:

```{r, eval = FALSE}
b1 = barplot(tx,ylim=c(0,0.23),ylab="Probability")
``` 

**4.** Add the theoretical values, plotting them
at the same $x$-locations as the centers
of the bars:
```{r, eval = FALSE}
points(b1,dbinom(0:12,prob=p,size=N),pch=16)
```
(`barplot()` doesn't put the bars
at $x$ locations corresponding to their
numerical values, so you have to save
those values as `b1` and re-use
them to make sure the theoretical
values end up in the right place.)

**5.** A few alternative ways to do this plot would be:

    5.1. `plot(factor(x)); points(b1,10000*dbinom(0:12,prob=p,size=N))` 
(plots the number of observations without rescaling and
scales the probability distribution to match);

    5.2. `plot(table(x)/10000); points(0:12,dbinom(0:12,prob=p,size=N))` 
Plotting a table does a plot with `type="h"` (high
density), which plots a vertical line for each value.
I think it's not quite as pretty as the barplot, but
it works.
Unlike factors, tables can be scaled numerically,
and the lines end up at the right numerical locations,
so we can just use `0:12` as the $x$ locations
for the theoretical values.

    5.3. You could also draw a histogram:
since histograms were really designed
continuous data you have to make sure the
breaks occur in the right place (halfway
between counts):

```{r,eval=FALSE}
h = hist(x,breaks=seq(-0.5,12.5,by=1),col="gray",
       prob=TRUE)
points(0:12,dbinom(0:12,prob=p,size=N))
```

Doing the equivalent plot for continuous distributions is actually somewhat easier, since you don't have
to deal with the complications of a discrete distribution: just use `hist(...,prob=TRUE)` to show the
sampled distribution (possibly with `ylim` adjusted for the maximum of the theoretical density distribution) and `ddist(x,[parameters],add=TRUE)` to add the theoretical curve (e.g.: `curve(dgamma(x,shape=2,scale=1,add=FALSE))`).

**Exercise 2***:
Pick 10,000 negative binomial
deviates with $\mu=2$, $k=0.5$
(using `rnbinom()`).
Pick one of the ways above to draw
the distribution.  
Check that the mean and variance
agree reasonably well with the theoretical values.
Add points representing
the theoretical distribution to the plot.
Now translate the $\mu$ and $k$
parameters into their $p$ and $nn$
equivalents (the coin-flipping derivation
of the negative binomial), and add
those points to the plot [use a different
plotting symbol to make sure you can see
that they overlap with the theoretical
values based on the $\mu$, k parameterization].

# 2. Averaging across discrete and continuous distributions

Suppose we have a (tiny) data set;
we can organize it in two different ways,
in standard long format or in tabular form:
```{r}
dat = c(5,6,5,7,5,8); dat
tabdat=table(dat); tabdat
``` 
To get the (sample) probability distribution of the data,
just scale by the total sample size:
```{r}
prob=tabdat/length(dat); prob
``` 
(dividing by `sum(tabdat)` would
be equivalent).

In the long format, we can take
the mean with `mean(dat)`
or, replicating the formula $\sum x_i/N$
exactly, `sum(dat)/length(dat)`.

In the tabular format, we
can calculate the mean 
with the formula $\sum P(x) x$,
which in `R` would be
`sum(prob*5:8)` or
more generally
```{r}
vals = as.numeric(names(prob))
sum(prob*vals)
``` 
(you could also get the values
by `as.numeric(levels(prob))`,
or by `sort(unique(dat))`).

However, `mean(prob)`
or `mean(tabdat)` is just plain wrong
(at least, I can't think of a situation where
you would want to calculate this value).

**Exercise 3:** figure out
what it means that
`mean(tabdat)` equals `r mean(tabdat)`.

Going back the other way, from a table to raw values, we can use
the `rep()` function to repeat values an appropriate number of times.
In its simplest form, `rep(x,n)` just creates
a vector repeats `x` (which
may be either a single value or a vector) `n` times, but
**if n is a vector as well** then each element of `x`
is repeated the corresponding number of times: for example,
```{r}
rep(c(1,2,3),c(2,1,5))
``` 
gives two copies of 1, one copy of 2, and five copies of 3.

Therefore,
```{r}
rep(vals,tabdat)
``` 
will recover our original data (although not in the original
order) by repeating each element of `vals`
the correct number of times.

## 2.1. Jensen's inequality

Jensen's inequality states the following: Suppose 
you have a number of values, $x$, with a mean $\bar{x}$, and a non-linear 
function $f(x)$. Then the mean of $f(x)$ is not equal to $f(\bar{x})$.

Jensen's inequality can be important in a number of cases. The first one is mentioned in Ch. 4 (page 104) how variability can change the mean behaviour of a system (damselfish). Another example where Jensen's inequality kicks in is when transforming your data. Data-transformations are commonly applied to get normally distributed errors. 

**Exercise 4**:
In statistical models, you often estimate the mean effect of a given treatment. Find out what the effect of Jensen's inequality is on a series of log-tranformed datapoints with respect to the estimated mean.

Use the following pseudo-code:
1. Generate 10 random deviates from a uniform distribution (choose the range of 0 to 10).
2. Calculate the mean of those 20 deviates.
3. Calculate the mean of the log-transformed values and transform this mean back the normal scale. 
4. Explain differences between the two means. 
  + _hint_: make a `log(x)` curve and pick two random points on the x-axis. 

**Exercise 5***:
try the above exercise again, but this time
with a gamma distribution instead of an exponential.
Keep the mean equal to 24.5 and change the
variance to 100, 25, and 1 (use the information
that the mean of the gamma distribution is `shape*scale`
and the variance is `shape*scale^2`).
Including the results for the exponential
(which is a gamma with shape=1), make a table
showing how the (1) true value of
mean recruitment [calculated by
numerical integration in `R` either
using `integrate()` or summing
over small $\Delta S$] (2) value of
recruitment at the mean settlement 
(3) delta-method approximation
(4,5) proportional error in \#2 and \#3
change with the variance.

# 3. The method of moments: reparameterizing distributions

In the chapter, I showed how to use the *method of moments*
to estimate the parameters of a distribution by setting the
sample mean and variance ($\bar x$, $s^2$) equal to the theoretical
mean and variance of a distribution and solving for the parameters.
For the negative binomial, in particular, I found $\mu=\bar x$
and $k=(\bar x)/(s^2/\bar x -1)$.

You can also define your own functions that use
your own parameterizations: call them `my_function`
rather than just replacing the standard `R` functions
(which will lead to insanity in the long run).

For example, defining
```{r}
my_dnbinom = function(x,mean,var,...) {
  mu = mean
  k = mean/(var/mean-1)
  dnbinom(x,mu=mu,size=k,...)
}

my_rnbinom = function(n,mean,var,...) {
  mu = mean
  k = mean/(var/mean-1)
  rnbinom(n,mu=mu,size=k,...)
}
``` 
(the `...` in the function takes any other arguments
you give to `my_dnbinom` and just passes them through,
unchanged, to `dnbinom`).

Defining your own functions can be handy if you need
to work on a regular basis with a distribution that
uses a different parameterization than the one built
into the standard `R` function.

You can use the kinds of histograms shown above to test your
results (remembering that the method of moments estimates
may be slightly biased especially for small samples --- but
they shouldn't cause errors as large as those caused by 
typical algebra mistakes).

```{r}
x = my_rnbinom(100000,mean=1,var=4)
mean(x)
var(x)
``` 

```{r}
tx = table(factor(x,levels=0:max(x)))/100000
b1 = barplot(tx,ylab="Probability")
points(b1,my_dnbinom(0:max(x),mean=1,var=4),pch=16)
abline(v=1)
``` 

**Exercise 6***:
Morris (1997) gives a definition of the beta function that
is different from the standard statistical parameterization.
The standard parameterization is
$$
\mbox{Beta}(x|a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} x^{a-1}(1-x)^{b-1}
$$
whereas Morris uses
$$
\mbox{Beta}(x|P,\theta) = \frac{\Gamma(\theta)}{\Gamma(\theta P)\Gamma(\theta (1-P))} x^{\theta P-1} (1-x)^{\theta(1-P)-1}.
$$
Find expressions for $P$ and $\theta$ in terms of $a$ and $b$
and vice versa.  Explain why you might prefer Morris's
parameterization.
Define a new set of functions that generate random
deviates from the beta distribution (`my_rbeta`)
and calculate the density function (`my_dbeta`)
in terms of $P$ and $\theta$.
Generate a histogram from this distribution
and draw a vertical line showing the mean of the distribution.

# 4. Creating new distributions

## 4.1. Zero-inflated distributions
The general formula for the probability distribution of
a zero-inflated distribution, with an underlying
distribution $P(x)$ and a zero-inflation probability
of $p_z$, is:
\begin{eqnarray*}
\mbox{Prob}(0) & = & p_z + (1-p_z) P(0) \\
\mbox{Prob}(x>0) & = & (1-p_z) P(x)
\end{eqnarray*}
So, for example, we could define a probability distribution
for a zero-inflated negative binomial as follows:
```{r}
dzinbinom = function(x,mu,size,zprob) {
  ifelse(x==0,
         zprob+(1-zprob)*dnbinom(0,mu=mu,size=size),
         (1-zprob)*dnbinom(x,mu=mu,size=size))
}
``` 
(the name, `dzinbinom`, follows the `R` convention
for a probability distribution function: a `d`
followed by the abbreviated name of the distribution,
in this case `zinbinom` for
"**z**ero-**i**nflated **n**egative
**binom**ial").

The `ifelse()` command checks every element of
`x` to see whether it is zero or not and fills
in the appropriate value depending on the answer.

A random deviate generator would look like this:
```{r}
rzinbinom = function(n,mu,size,zprob) {
  ifelse(runif(n)<zprob,
         0,
         rnbinom(n,mu=mu,size=size))
}
``` 
The command `runif(n)` picks `n`
random values between 0 and 1; the `ifelse`
command compares them with the value of
`zprob`.  If an individual value is
less than `zprob` (which happens with probability
`zprob`=$p_z$), then the corresponding
random number is zero; otherwise it is a value
picked out of the appropriate negative binomial
distribution.

**Exercise 7**:
check graphically that these functions actually
work. For an extra challenge, calculate the mean
and variance of the zero-inflated negative binomial
and compare it to the results of
`rzinbinom(10000,mu=4,size=0.5,zprob=0.2)`.

# 5. Compounding distributions

The key to compounding distributions in `R` is
that the functions that generate random deviates
can all take a vector of different parameters
rather than a single parameter.  For example,
if you were simulating the number of hatchlings
surviving (with individual probability 0.8)
from a series of 8 clutches, all of size
10, you would say
```{r}
rbinom(8,size=10,prob=0.8)
``` 
but if you had a series of clutches
of different sizes, you could still
pick all the random values at the same time:
```{r}
clutch_size = c(10,9,9,12,10,10,8,11)
rbinom(8,size=clutch_size,prob=0.8)
``` 

Taking this a step farther, the clutch size
itself could be a random variable:
```{r}
clutch_size = rpois(8,lambda=10)
rbinom(8,size=clutch_size,prob=0.8)
``` 
We've just generated a Poisson-binomial
random deviate ...

As a second example, I'll 
follow Clark *et al.* in constructing
a distribution that is a compounding
of normal distributions, with 1/variance of each sample drawn from
a gamma distribution.

First pick the variances as the reciprocals of 10,000 values
from a gamma distribution with shape 5 (setting the scale equal
to 1/5 so the mean will be 1):
```{r}
var_vals=1/rgamma(10000,shape=5,scale=1/5)
``` 

Take the square root, since `dnorm` uses
the standard deviation and not the variance
as a parameter:
```{r}
sd_vals = sqrt(var_vals)
```

Generate 10,000 normal deviates using this
range of standard deviations:
```{r}
x = rnorm(10000,mean=0,sd=sd_vals)
``` 

Figure 4 shows a histogram of the
following commands:
```{r,eval=FALSE}
hist(x,prob=TRUE,breaks=100,col="gray")
curve(dt(x,df=11),add=TRUE,lwd=2)
``` 

```{r,fig=TRUE, fig.cap="Clark model: inverse gamma compounded with normal, equivalent to the Student $t$ distribution"}
hist(x,prob=TRUE,breaks=100,col="gray")
curve(dt(x,df=11),add=TRUE,lwd=2)
``` 

The superimposed curve is a $t$ distribution
with 11 degrees of freedom; it turns out that
if the underlying gamma distribution has 
shape parameter $p$, the resulting $t$ distribution
has $df=2p+1$.
(Figuring out the analytical form of the compounded
probability distribution or density function, or
its equivalence to some existing distribution,
is the hard part; for the most part, though,
you can find these answers in the ecological
and statistical literature if you search hard
enough.

**Exercise 8***:
generate 10,000 values from
a gamma-Poisson compounded distribution
with parameters shape=$k=0.5$, scale=$\mu/k=4/0.5=8$
and demonstrate that it's equivalent to a
negative binomial with the appropriate
$\mu$ and $k$ parameters.

*Extra credit*: generate 10,000
values from a lognormal-Poisson distribution
with the same expected mean and variance
(the variance of the lognormal should
equal the variance of the gamma distribution
you used as a compounding distribution;
you will have to do some algebra
to figure out the values of `meanlog`
and `sdlog` needed to produce a lognormal
with a specified mean and variance.
Plot the distribution and superimpose
the theoretical distribution of the
negative binomial with the same mean
and variance
to see how different the shapes of the distributions
are.


# 6. Choosing error distributions 
In this exercise we revisit exercise 6 of lab 3.
In that exercise we had six different datasets each describing a biological phenonmenon. The first step to model your data is to choose a deterministic function that describes the mean effect of the predictor variable (x) on the response variable (y). The second step involves the choice of the stochastic distribution which describes how the data varies around the mean. 
**Exercise 9** Reload the six datasets and choose for each dataset one or two candidate error distributions.
  _hint_: ask yourself a number of questions: Do I have counts (integer) or continuous (real) values? Do I have positive values only or also negative values? 