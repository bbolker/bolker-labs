---
title: "Additional exercise Ch. 6"
author: "Bob Douma & Alejandro Morales"
date: "22 November 2017"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

## Assessing the confidence limits of parameter estimates

This exercise will be about assessing the uncertainty in the estimated parameters. Bolker recommends to use the likelihood profile for assessing the uncertainty in the parameters because this one is more accurate than the one based on the Hessian matrix. We take a number of steps to get a feel for the uncertainty in the parameter estimates.

Take the first dataset of the six datasets you have worked with earlier on. Assume that the function was generated by the monomolecular function $a(1-e^{(-bx)}$. Fit this model with normally distributed errors through this data with `mle2` and inspect the coefficients.

1. Calculate for the parameters $a$ and $b$ the NLL for a number of combinations of $a$ and $b$ and plot the NLL surface using `contour` plot. See Bolker Lab 6 for inspiration on coding. What is your conclusion about the relationship between the two parameters? Are they fully independent?
  _hint_: you can use a for a double for-loop to run over all parameters
  _hint_: check Lab 6 of Bolker for inspiration how to do this.
  
2.  Calculate the confidence intervals of the parameters through constructing the likelihood profile. Consult page 106 of Bokler or Lab 6 for how to calculate the confidence intervals based on the likelihood profile and through the quadratic approximation. Use the following pseudocode to achieve this:
  a. Adapt the likelihood function such that one parameter is not optimised but chosen by you, say parameter $a$. 
  b .Vary $a$ of a range and optimise the other parameteters. 
  c. Plot the NLL as a function of parameter $a$.
  d. Find the values of $a$ that enclose $-L + \chi^2(1-\alpha)/2$
  e. Compare your results with the function `confint()`
  
3. Calculate the confidence intervals through the quadratic approximation. Take the following steps to achieve this:
  a. Get the standard error of the parameter estimates through `vcov`. Note that `vcov` return the variance/covariance matrix
  b. Calculate the interval based on the fact that the 95% limits are 1.96 (qnorm(0.975,0,1)) standard deviation units away from the mean.

4. Plot the confidence limits of the both method and compare the results. What is your conclusion?

```{r,eval=F,echo=F}
shapes1= read.csv("shapes1.csv")
plot(shapes1)

nll.mle = function(a,b,sd){
  # this calculates the mean y for a given value of x: the deterministic function
  mu = a*(1-exp(-b*shapes1$x))
  # this calculates the likelihood of the function given the probability 
  # distribution, the data and mu and sd
  nll = -sum(dnorm(shapes1$y,mean=mu,sd=sd,log=T)) 
  return(nll)
}

library(bbmle)

# Try 4 different starting points
mle2.1 = vector("list", 4)
start_a = c(5,10,20,30)
start_b = c(0.001,0.005,0.01,0.1)
for(i in 1:4) {
  mle2.1[[i]] = mle2(nll.mle,start=list(a=start_a[i],b = start_b[i], sd=1), method="Nelder-Mead") 
}

# Check the best fit (in this case it is 3rd starting point)
for(i in 1:4) {
  print(logLik(mle2.1[[i]]))
}

# Extract the best fit for the rest of the analysis
best_mle2.1 = mle2.1[[3]]
summary(best_mle2.1)
logLik(best_mle2.1)
confint(best_mle2.1)
coef(best_mle2.1)

plot(shapes1)
curve(coef(best_mle2.1)[1]*(1-exp(-coef(best_mle2.1)[2]*x)),add=T)
curve(coef(mle2.1[[1]])[1]*(1-exp(-coef(mle2.1[[1]])[2]*x)),add=T, col = 2)
```

```{r,eval=F,echo=F}
# likelihood surface
a1 = seq(0,40,length.out = 100)
b1.1 = seq(0.001,0.03,length.out=100)
b1.2 = seq(0.1,10,length.out=100)

nll.grid = expand.grid(a1,b1.1)
nll.grid$NLL = NA
no = 0
# Construct first contour
for (i in 1:length(a1)){
  for (j in 1:length(b1.1)){
    no = no + 1
    nll.grid[no,1] = a1[i]
    nll.grid[no,2] = b1.1[j]
    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.1[j],sd=2.06)
  }
}
library(reshape2)
z1.1 = as.matrix(dcast(nll.grid,Var1~Var2)[,-1])

# Construct second contour
no = 0
for (i in 1:length(a1)){
  for (j in 1:length(b1.2)){
    no = no + 1
    nll.grid[no,1] = a1[i]
    nll.grid[no,2] = b1.2[j]
    nll.grid[no,3] = nll.mle(a=a1[i],b=b1.2[j],sd=2.06)
  }
}
z1.2 = as.matrix(dcast(nll.grid,Var1~Var2)[,-1])

# Plot the two contours
par(mfrow = c(2,1), mar = c(0,4,1,1), las = 1)
contour(a1,b1.2,z1.2,nlevels = 20, xaxt = "n", yaxt = "n", ylim = c(0,9))
axis(2, seq(1,9,2))
points(start_a[4],start_b[4],pch=4, col = 4, lwd = 2)
points(coef(mle2.1[[1]])[1],coef(mle2.1[[1]])[2],pch=19, col = 2)
points(coef(mle2.1[[2]])[1],coef(mle2.1[[2]])[2],pch=19, col = 3)
points(coef(mle2.1[[4]])[1],coef(mle2.1[[4]])[2],pch=19, col = 4)
contour(a1,b1.2,z1.2,levels=120,col=2,add=T)

par(mar = c(3.5,4,0.5,1))
contour(a1,b1.1,z1.1,nlevels = 20)
points(coef(best_mle2.1)[1],coef(best_mle2.1)[2],pch=19)
points(start_a[1],start_b[1],pch=4, col = 2, lwd = 2)
points(start_a[2],start_b[2],pch=4, col = 3, lwd = 2)
points(start_a[3],start_b[3],pch=4, col = 1, lwd = 2)
contour(a1,b1.1,z1.1,levels=120,col=2,add=T)



# profile
nll.mle1 = function(a,sd){
  # this calculates the mean y for a given value of x: the deterministic function
  mu = a*(1-exp(-b*x))
  # this calculates the likelihood of the function given the probability 
  # distribution, the data and mu and sd
  nll = -sum(dnorm(y,mean=mu,sd=sd,log=T)) 
  return(nll)
}

nll = numeric(length(b1.1))
for (i in 1:length(b1.1)){
  b = b1.1[i]
  mle.21 = mle2(nll.mle1,start=list(a=25,sd=7.96),data=data.frame(x=shapes1$x,y=shapes1$y),method="Nelder-Mead")   
  nll[i] = -logLik(mle.21)
}
par(mfrow = c(1,1))
plot(nll~ b1.1,type="l",xlim=c(0.008,0.012), ylim = c(117,125))
which.min(nll)

# cutoff
-logLik(best_mle2.1) + qchisq(0.95, 1)/2
which(nll < 119.852)
b1.1[c(23,35)]

plot(nll~ b1.1,type="l",xlim=c(0.0070,0.012),ylim=c(116,125))
abline(v=c(0.00744,0.01096),lty=2)
abline(v=0.008968,lty=1,lwd=2)
abline(v=c(0.00738,0.01103),lty=2,col="red")

se.mu = sqrt(diag(solve(best_mle2.1@details$hessian))[2])
b + c(-1,1)*qnorm(0.975) * se.mu
confint(best_mle2.1)

abline(v=c(0.007177,0.0107589),col="blue")

```

\newpage
## Hints for choosing deterministic functions and stochastic functions
  
  1. Deterministic functions

    **dataset 1**
    light response curve. There are a number of options of functions to choose from, depending on the level of sophistication:
    $\frac{ax}{(b+x)}$, $a(1-e^{(-bx)})$, $\frac{1}{2\theta}(\alpha I+p_{max}-\sqrt(\alpha I+p_{max})^2-4\theta I p_{max})$ see page 98. A parameter `d` can be added in all cases to shift the curve up or down.
      
    **dataset 2**
    The dataset describes a functional responses. Bolker mentions four of those $min(ax,s)$ $\frac{ax}{(b+x)}$, $\frac{ax^2}{(b^2+x^2)}$,$\frac{ax^2}{(b+cx+x^2)}$
      
    **dataset 3**
    Allometric relationships generally have the form $ax^b$
      
      **dataset 4**
      This could be logistic growth $n(t)=\frac{K}{1+(\frac{K}{n_0})e^{-rt}}$ or the gompertz function $f(x)=e^{-ae^{-bx}}$
      
      **dataset 5**
      What about a negative exponential? $ae{-bx}$ or a power function $ax^b$
      
      **dataset 6**
      Species reponse curves are curves that describe the probability of presence as a function of some factor. A good candidate good be a unimodel response curve. You could take the equation of the normal distribution without the scaling constant: e.g.
      $a e^{\frac{-(x-\mu)^2}{2\sigma^2}}$

  2. Stochastic functions/Probability distributions

      **dataset 1**
      y represents real numbers and both positive and negative numbers occur. This implies that we should choose a continuous         probability distribution. In addition, the numbers seems unbound. Within the family of continuous probability distributions, the normal seems a good candidate distribution because this one runs from -$\inf$ to +$\inf$. In contrast the Gamma and the Lognormal only can take positive numbers, so these distributions cannot handle the negative numbers. In addition, the beta distribution is not a good candidate because it runs from 0-1.

      **dataset 2**
      y represents real numbers and only positive numbers occur. The data represents a functional response (intake rate of the predator), and it is likely that you can only measure positive numbers (number of prey items per unit of time).  This implies that we should choose a continuous probability distribution. Within the family of continuous probability distributions, the Gamma and the Lognormal could be taken as candidate distributions because they can only take positive numbers (beware that the Gamma cannot take 0). However, you could try to use a normal as well.

      **dataset 3**
      y seems represents counts (this is the cone dataset that is introduced in ch. 6.). Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.
      
      **dataset 4**
      y represents population size over time. From looking at the data, they seems to represent counts. Given that it contains counts we can pick a distribution from the family of discrete distributions. The Poisson and the Negative Binomial could be good candidates to describe this type of data.  

      **dataset 5**
      No information is given on y. The data clearly seems to represent counts. Thus the same reasoning applies here as to the two previous datasets.
      
      **dataset 6**
      The data (y) represents species occurences (presence/absence). The binomial model would be a good model to predict the probability of presence.  